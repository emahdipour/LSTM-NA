{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project implemented by Elham Mahdipour\n",
    "## She is a Ph.D. Candidate of computer engineering at Yazd University, Yazd, Iran.\n",
    "### Please feel free and contact to me: elham.mahdipour@gmail.com/ elham.mahdipour@stu.yazd.ac.ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 \n",
    "## Create Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x2819f0f86a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "G1=nx.read_weighted_edgelist('large dataset\\sc-sc.evals')\n",
    "G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x281a1ad0278>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2=nx.read_weighted_edgelist('large dataset\\ec-ec.evals')\n",
    "G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265\n",
      "6613\n"
     ]
    }
   ],
   "source": [
    "### Check and Swap if G1 > G2 ###\n",
    "if len(G1)>len(G2):\n",
    "    temp=G1\n",
    "    G1=G2\n",
    "    G2=temp\n",
    "print(len(G1))\n",
    "print(len(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x281a2126588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_target_na=nx.read_weighted_edgelist('large dataset\\ec-sc.evals')\n",
    "G_target_na  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed1=G1.edges()\n",
    "ed2=G2.edges()\n",
    "\n",
    "nd1=G1.nodes()\n",
    "nd2=G2.nodes()\n",
    "\n",
    "el1=list(ed1)\n",
    "el2=list(ed2)\n",
    "\n",
    "nd1=list(nd1)\n",
    "nd2=list(nd2)\n",
    "\n",
    "degG1 = [val for (node, val) in G1.degree()]\n",
    "degG2 = [val for (node, val) in G2.degree()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute score for create similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deg_Diff(G1,G2):\n",
    "    Degree_Difference=np.zeros((len(G1),len(G2)))\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            Degree_Difference[i][j]=abs(degG1[i]-degG2[j])/max(degG1[i],degG2[j])\n",
    "    return Degree_Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_pageRank(X):\n",
    "    a=nx.pagerank(X)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_pagerank(x,y):  #x is G1, y is G2\n",
    "   # print(len(x))\n",
    "    p1=score_pageRank(x)\n",
    "    b=p1.values()\n",
    "    pr1=list(b)\n",
    "    p2=score_pageRank(y)\n",
    "    c=p2.values()\n",
    "    pr2=list(c)\n",
    "    pr=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            #print(pr1[i],pr2[j])\n",
    "            pr[i][j]=abs(pr1[i]-pr2[j])/max(pr1[i],pr2[j])   #minimum pr is maximum similarity of topology \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_edges(index_node, G, GraphNumber):\n",
    "    if GraphNumber==1:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd1[index_node]):                       \n",
    "            sum_edge=sum_edge+degG1[nd1.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG1[index_node]-1) if degG1[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG1[index_node]*temp)\n",
    "    else:\n",
    "        sum_edge=0        \n",
    "        for i in G.neighbors(nd2[index_node]):                       \n",
    "            sum_edge=sum_edge+degG2[nd2.index(i)]\n",
    "        #print(sum_edge)\n",
    "        temp=(degG2[index_node]-1) if degG2[index_node]> 1 else 1        \n",
    "        coeff_node=(2*sum_edge)/(degG2[index_node]*temp)\n",
    "    return coeff_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Ea(G1,G2):\n",
    "    Ea_G1=np.zeros(len(G1))\n",
    "    Ea_G2=np.zeros(len(G2))\n",
    "    for i in range(len(G1)):\n",
    "        Ea_G1[i]=coefficient_edges(i, G1, 1)\n",
    "    for j in range(len(G2)):\n",
    "        Ea_G2[j]=coefficient_edges(j,G2,2)\n",
    "    ea=[Ea_G1, Ea_G2]\n",
    "    return(ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute relative clustering coefficient difference between node a (in G1) and node b (in G2)\n",
    "def CD(G1, G2):\n",
    "    cd=np.zeros((len(G1),len(G2)))\n",
    "    EA=compute_Ea(G1,G2)\n",
    "    #print(EA[0])      #Ea for G1\n",
    "    #print(\"===================\")\n",
    "    #print(EA[1])      #Ea for G2\n",
    "    for i in range(len(G1)):\n",
    "        for j in range(len(G2)):\n",
    "            cd[i,j]=abs(EA[0][i]-EA[1][j])/max(EA[0][i],EA[1][j])\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_score(x,y):\n",
    "    seq=np.zeros((len(x),len(y)))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            q1=G_target_na.get_edge_data(str(nd1[i]),str(nd2[j]))\n",
    "            if q1==None:\n",
    "                c=0\n",
    "            else:\n",
    "                c=list(q1.values())\n",
    "                c=c[0]\n",
    "            seq[i][j]=c    \n",
    "            \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(G1,G2):\n",
    "    coeff_pr=coefficient_pagerank(G1,G2)\n",
    "    dd=deg_Diff(G1,G2)\n",
    "    cd=CD(G1,G2)\n",
    "    seq_sc=sequence_score(G1,G2)\n",
    "    \n",
    "    alpha=0.1\n",
    "    betta=0.2\n",
    "    gamma=0.2\n",
    "    zetta=1-alpha-betta-gamma\n",
    "    s=alpha*(1-coeff_pr)+betta*(1-dd)+gamma*(1-cd)+zetta*seq_sc\n",
    "    return s,coeff_pr, dd, cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m, coeff_pr, dd, cd=compute_score(G1,G2)\n",
    "sim=m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change problem to classification \n",
    "## [node of G1, node of G2, Coefficient page rank, clustering coefficient difference,  similarity score, alignment=yes(0)/TN or no(1)/TP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "int_nd1=np.zeros(len(nd1))\n",
    "int_nd2=np.zeros(len(nd2))\n",
    "\n",
    "species=['ec','sc','ce','dm','mm','hs']\n",
    "ch1=0  #please set index for first species of species list, for example index of ec is 0\n",
    "ch2=1  #please set index for second species of species list, for example index of sc is 1\n",
    "# If don't set index with considering species may be given an error\n",
    "for i in range(len(nd1)):\n",
    "    if (species[ch1] in nd1[i] or species[ch2] in nd1[i]):\n",
    "        s=nd1[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd1[i]=x    \n",
    "for i in range(len(nd2)):\n",
    "    if (species[ch1] in nd2[i] or species[ch2] in nd2[i]):\n",
    "        s=nd2[i][2:]\n",
    "        x=int(s)\n",
    "        int_nd2[i]=x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28204445\n"
     ]
    }
   ],
   "source": [
    "# en_mat is encoding matrix\n",
    "en_mat=[]\n",
    "\n",
    "for i in range(len(nd1)):\n",
    "    for j in range(len(nd2)):\n",
    "        if G_target_na.has_edge(nd1[i],nd2[j]):\n",
    "            align_class='Yes'\n",
    "        else:\n",
    "            align_class='No'\n",
    "        \n",
    "        sample=[int_nd1[i],int_nd2[j], coeff_pr[i][j], dd[i][j],cd[i][j],sim[i][j],align_class] \n",
    "        en_mat.append(sample)\n",
    "print(len(en_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4561 28199884\n"
     ]
    }
   ],
   "source": [
    "yc=[]\n",
    "noc=[]\n",
    "for i in range(len(en_mat)):\n",
    "    if en_mat[i][6]=='Yes':\n",
    "        yc.append(en_mat[i])\n",
    "    else:\n",
    "        noc.append(en_mat[i])\n",
    "print(len(yc), len(noc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28204445, 28204445)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=yc+noc \n",
    "len(data)\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(data)):\n",
    "    X.append(data[i][0:6])\n",
    "    y.append(data[i][6])\n",
    "\n",
    "len(X),len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-sc-data.pickle', 'wb') as f:\n",
    "    pickle.dump([X_train, y_train,X_test,y_test],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('large dataset/ec-sc-data.pickle','rb') as f:\n",
    "    X_train, y_train,X_test,y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000 25384000 2820445 2820445\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,y_tr,x_te,y_te=X_train, y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve data Imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'No': 25379873, 'Yes': 4127})\n",
      "Resampled dataset shape Counter({'No': 25379873, 'Yes': 25379873})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "#sampling_strategy = 'not minority'\n",
    "smt = SMOTEENN(random_state=42) #sampling_strategy=sampling_strategy)\n",
    "\n",
    "X_res_train, y_res_train = smt.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_res_train))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'No': 2820011, 'Yes': 434})\n",
      "Resampled dataset shape Counter({'No': 2820011, 'Yes': 2820011})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y_test))\n",
    "smt = SMOTEENN(random_state=42)\n",
    "X_res_test, y_res_test = smt.fit_resample(X_test, y_test)\n",
    "print('Resampled dataset shape %s' % Counter(y_res_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_res_train=np.array(X_res_train)\n",
    "y_res_train=np.array(y_res_train)\n",
    "X_res_test=np.array(X_res_test)\n",
    "y_res_test=np.array(y_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_res_train)\n",
    "encoded_Y = encoder.transform(y_res_train)\n",
    "# One Hot Encode\n",
    "y_res_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_res_test)\n",
    "encoded_Y = encoder.transform(y_res_test)\n",
    "# One Hot Encode\n",
    "y_res_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# One Hot Encode\n",
    "y_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode Class (Species)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "# One Hot Encode\n",
    "y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Architecture of LSTM-NA Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 8)           48        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4)                 208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 266\n",
      "Trainable params: 266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 40607796 samples, validate on 10151950 samples\n",
      "Epoch 1/5\n",
      "40607796/40607796 [==============================] - 8332s 205us/step - loss: 9.9120e-04 - acc: 0.9997 - mae: 8.0694e-04 - mse: 2.2956e-04 - val_loss: 2.5870e-05 - val_acc: 1.0000 - val_mae: 2.5837e-05 - val_mse: 6.6947e-10\n",
      "Epoch 2/5\n",
      "40607796/40607796 [==============================] - 8237s 203us/step - loss: 0.0011 - acc: 1.0000 - mae: 0.0011 - mse: 3.8044e-05 - val_loss: 5.5153e-05 - val_acc: 1.0000 - val_mae: 4.0447e-04 - val_mse: 1.6391e-07\n",
      "Epoch 3/5\n",
      "40607796/40607796 [==============================] - 8561s 211us/step - loss: 1.6731e-04 - acc: 1.0000 - mae: 1.6729e-04 - mse: 3.4194e-08 - val_loss: 2.0015e-05 - val_acc: 1.0000 - val_mae: 1.4747e-04 - val_mse: 2.1545e-08\n",
      "Epoch 4/5\n",
      "40607796/40607796 [==============================] - 8530s 210us/step - loss: 3.1597e-05 - acc: 1.0000 - mae: 3.1595e-05 - mse: 1.9105e-09 - val_loss: 2.4351e-07 - val_acc: 1.0000 - val_mae: 1.7681e-06 - val_mse: 3.1391e-12\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from keras import layers\n",
    "import keras\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]\n",
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(layers.Embedding(6, 8))\n",
    "model_lstm.add(layers.LSTM(4))\n",
    "model_lstm.add(Dense(2, activation='softmax'))\n",
    "model_lstm.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc','mae','mse'])\n",
    "model_lstm.summary()\n",
    "history_lstm = model_lstm.fit(X_res_train, y_res_train,epochs=5,validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import h5py\n",
    "model_lstm.save('large dataset/deep_model_resample_6features_lstm_ec-sc.h5')\n",
    "model_lstm.save_weights('large dataset/deep_model_resample_6features_lstm_weights_ec-sc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test tune model for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#del model_onelstm\n",
    "model_onelstm = load_model('large dataset/deep_model_resample_6features_lstm_ec-sc.h5')\n",
    "model_onelstm.load_weights('large dataset/deep_model_resample_6features_lstm_weights_ec-sc.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000/25384000 [==============================] - 1955s 77us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0015719945348636034, 1.0, 0.0015777189983054996, 2.4809851311147213e-06]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_tr = model_onelstm.evaluate(X_train, y_train)\n",
    "result_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       ...,\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_tr=model_onelstm.predict(X_train)\n",
    "out_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25379886        0]\n",
      " [       0     4114]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train.argmax(axis=1), out_tr.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820445/2820445 [==============================] - 215s 76us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0015719860673934574, 1.0, 0.0015716645866632462, 2.465048964950256e-06]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_te = model_onelstm.evaluate(X_test, y_test)\n",
    "result_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       ...,\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033],\n",
       "       [0.9984296 , 0.00157033]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_te=model_onelstm.predict(X_test)\n",
    "out_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2819998       0]\n",
      " [      0     447]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), out_te.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM model for real data (without resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 8)           48        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4)                 208       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 266\n",
      "Trainable params: 266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20307200 samples, validate on 5076800 samples\n",
      "Epoch 1/2\n",
      "20307200/20307200 [==============================] - 3979s 196us/step - loss: 0.0016 - acc: 0.9999 - mae: 5.7148e-04 - mse: 1.8825e-04 - val_loss: 0.0011 - val_acc: 0.9998 - val_mae: 3.0702e-04 - val_mse: 1.5160e-04\n",
      "Epoch 2/2\n",
      "20307200/20307200 [==============================] - 3954s 195us/step - loss: 0.0016 - acc: 0.9999 - mae: 4.4753e-04 - mse: 1.6095e-04 - val_loss: 0.0015 - val_acc: 0.9998 - val_mae: 3.2360e-04 - val_mse: 1.5201e-04\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from keras import layers\n",
    "import keras\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]\n",
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(layers.Embedding(6, 8))\n",
    "model_lstm.add(layers.LSTM(4))\n",
    "model_lstm.add(Dense(2, activation='softmax'))\n",
    "model_lstm.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['acc','mae','mse'])\n",
    "model_lstm.summary()\n",
    "history_lstm = model_lstm.fit(X_train, y_train,epochs=2,validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import h5py\n",
    "model_lstm.save('large dataset/deep_model_before_resample_6features_lstm_ec-sc.h5')\n",
    "model_lstm.save_weights('large dataset/deep_model_before_resample_6features_lstm_weights_ec-sc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tune model for real data (without resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#del model\n",
    "model_lstm = load_model('large dataset/deep_model_before_resample_6features_lstm_ec-sc.h5')\n",
    "model_lstm.load_weights('large dataset/deep_model_before_resample_6features_lstm_weights_ec-sc.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25384000/25384000 [==============================] - 2011s 79us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0017920834103681668,\n",
       " 0.99989253282547,\n",
       " 0.0007473152363672853,\n",
       " 0.00016190006863325834]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_tr = model_lstm.evaluate(X_train, y_train)\n",
    "result_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       ...,\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_tr=model_lstm.predict(X_train)\n",
    "out_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25379886        0]\n",
      " [    4114        0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_train.argmax(axis=1), out_tr.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999189647021746\n",
      "0.5\n",
      "0.49995947906746147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_train.argmax(axis=1), out_tr.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2820445/2820445 [==============================] - 214s 76us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.001765404671098096,\n",
       " 0.9998415112495422,\n",
       " 0.0007448680698871613,\n",
       " 0.000158529554028064]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without resample\n",
    "result_te = model_lstm.evaluate(X_test, y_test)\n",
    "result_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       ...,\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858662e-04],\n",
       "       [9.994142e-01, 5.858660e-04]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "out_te=model_lstm.predict(X_test)\n",
    "out_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2819998       0]\n",
      " [    447       0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test.argmax(axis=1), out_te.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49992075718547957\n",
      "0.5\n",
      "0.49996037545277916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(precision_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(recall_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))\n",
    "print(f1_score(y_test.argmax(axis=1), out_te.argmax(axis=1) , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test other classifier without resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tra,y_tra,x_tes,y_tes=X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=x_tr,y_tr,x_te,y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA classifier on training set: 1.00\n",
      "Accuracy of LDA classifier on test set: 1.00\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X_train, y_train)))\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999761660888749\n",
      "[[25379886        0]\n",
      " [     605     3509]]\n",
      "0.9999880813968492\n",
      "0.9264705882352942\n",
      "0.9603115009448575\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds_tr = lda.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999726993435433\n",
      "[[2819998       0]\n",
      " [     77     370]]\n",
      "0.9999863478808187\n",
      "0.9138702460850112\n",
      "0.952869550836202\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds = lda.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "# time: one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999808934762054\n",
      "[[25379886        0]\n",
      " [     485     3629]]\n",
      "0.9999904453721342\n",
      "0.9410549343704424\n",
      "0.9686766122826788\n",
      "Wall time: 20min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix\n",
    "# Make predictions\n",
    "preds_tr = knn.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999709265736435\n",
      "[[2819998       0]\n",
      " [     82     365]]\n",
      "0.9999854614053503\n",
      "0.9082774049217002\n",
      "0.9495001197595498\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds =  knn.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 1.00\n",
      "Accuracy of SVM classifier on test set: 1.00\n",
      "Wall time: 3h 29min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X_train, y_train)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.99998262685156\n",
      "[[25379886        0]\n",
      " [     441     3673]]\n",
      "0.999991312168673\n",
      "0.9464025279533301\n",
      "0.9716792312360152\n",
      "Wall time: 42min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds_tr = svm.predict(X_train)\n",
    "print(preds_tr)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_train, preds_tr))\n",
    "\n",
    "print(confusion_matrix(y_train, preds_tr))\n",
    "print(precision_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(recall_score(y_train, preds_tr , average=\"macro\"))\n",
    "print(f1_score(y_train, preds_tr , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'No' 'No' ... 'No' 'No' 'No']\n",
      "0.9999840450709019\n",
      "[[2819998       0]\n",
      " [     45     402]]\n",
      "0.9999920213982553\n",
      "0.9496644295302014\n",
      "0.973494243882846\n",
      "Wall time: 4min 42s\n",
      "Parser   : 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# Make predictions\n",
    "preds = svm.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(precision_score(y_test, preds , average=\"macro\"))\n",
    "print(recall_score(y_test, preds , average=\"macro\"))\n",
    "print(f1_score(y_test, preds , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
